{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#debezium-iceberg-consumer","title":"Debezium Iceberg Consumer","text":"<p>This project implements Debezium Server Iceberg consumer see Debezium Server. It enables real-time replication of Change Data Capture (CDC) events from any database to Iceberg tables. Without requiring Spark, Kafka or Streaming platform in between.</p> <p>See the Documentation Page for more details.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Requirements:</li> <li>JDK 21</li> <li>Maven</li> </ul>"},{"location":"#building-from-source-code","title":"Building from source code","text":"<pre><code>git clone https://github.com/memiiso/debezium-server-iceberg.git\ncd debezium-server-iceberg\nmvn -Passembly -Dmaven.test.skip package\n# unzip and run the application\nunzip debezium-server-iceberg-dist/target/debezium-server-iceberg-dist*.zip -d appdist\ncd appdist/debezium-server-iceberg\nmv config/application.properties.example config/application.properties\nbash run.sh\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>The Memiiso community welcomes anyone that wants to help out in any way, whether that includes reporting problems, helping with documentation, or contributing code changes to fix bugs, add tests, or implement new features. See contributing document for details.</p>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"contributing/","title":"Contributing","text":"<p>Debezium Iceberg consumer is a very young project and looking for new maintainers. There are definitively many small/big improvements to do, including documentation, adding new features to submitting bug reports.</p> <p>Please feel free to send pull request, report bugs or open feature request.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under Apache 2.0 License.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"How does the connector handle deletes, and what are the performance implications? <p>This connector writes data to Iceberg tables using the V2 specification. To optimize write performance, delete events are recorded in delete files, avoiding costly data file rewrites. While this approach significantly improves write performance, it can impact read performance, especially in <code>upsert</code> mode. In <code>append</code> mode, this performance trade-off is not applicable.</p> <p>To optimize read performance, you must run periodic table maintenance jobs to compact data and rewrite the delete files. This is especially critical for <code>upsert</code> mode.</p> Does the connector support schema evolution? <p>Full schema evolution, such as converting incompatible data types, is not currently supported. However, schema expansion\u2014including adding new fields or promoting field data types\u2014is supported. To enable this behavior, set the <code>debezium.sink.iceberg.allow-field-addition</code> configuration property to <code>true</code>.</p> <p>For a more robust way to handle schema changes, you can configure the connector to store all nested data in a <code>variant</code> field. This approach can seamlessly absorb many schema changes.</p> <pre><code># Store nested data in variant fields\ndebezium.sink.iceberg.nested-as-variant=true\n# Ensure event flattening is disabled (flattening is the default behavior)\ndebezium.transforms=,\n</code></pre> How can I replicate only specific tables from my source database? <p>By default, the Debezium connector replicates all tables in the database, which can result in unnecessary load. To avoid replicating tables you don't need, configure the <code>debezium.source.table.include.list</code> property to specify the exact tables to replicate. This will streamline your data pipeline and reduce overhead. For more details, refer to the [Debezium server source](https://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-property-table-include-list documentation.</p> How do I configure AWS S3 credentials? <p>You can set up AWS credentials in one of the following ways:</p> <ul> <li>In <code>application.properties</code>: Use the <code>debezium.sink.iceberg.fs.s3a.access.key</code> and <code>debezium.sink.iceberg.fs.s3a.secret.key</code> properties.</li> <li>As environment variables: Set <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</li> <li>Using Hadoop's configuration: Set up the <code>HADOOP_HOME</code> environment variable and add S3A configuration to <code>core-site.xml</code>. More information can be found here.</li> </ul>"},{"location":"iceberg/","title":"Debezium Iceberg Consumer","text":"<p>Directly replicate database Change Data Capture (CDC) events to Iceberg tables on cloud storage or HDFS, eliminating the need for intermediate systems like Spark, Kafka, or streaming platforms. </p>"},{"location":"iceberg/#iceberg-consumer","title":"<code>iceberg</code> Consumer","text":"<p>The Iceberg consumer replicates database Change Data Capture (CDC) events to target Iceberg tables. It supports both upsert and append modes for data replication.</p> <p>When event and key schema information is enabled (<code>debezium.format.value.schemas.enable=true</code> and <code>debezium.format.key.schemas.enable=true</code>), the destination Iceberg tables are automatically created upon the initial startup.</p>"},{"location":"iceberg/#configuration-properties","title":"Configuration properties","text":"Config Default Description <code>debezium.sink.type</code> <code>iceberg</code> Debezium server coniguration, must be set to <code>iceberg</code> activates the iceberg consumer. <code>debezium.sink.iceberg.warehouse</code> Root path of the Iceberg data warehouse <code>debezium.sink.iceberg.io-impl</code> <code>org.apache.iceberg.io.ResolvingFileIO</code> Iceberg file io implementation. Used to write iceberg data files. <code>debezium.sink.iceberg.catalog-name</code> <code>default</code> User-specified Iceberg catalog name. <code>debezium.sink.iceberg.table-namespace</code> <code>default</code> A namespace in the catalog. ex: <code>SELECT * FROM prod.db.table --&gt; catalog: prod, namespace: db, table: table</code> <code>debezium.sink.iceberg.table-prefix</code> `` Prefix for Iceberg tables. A prefix added to the names of Iceberg tables. <code>debezium.sink.iceberg.write.format.default</code> <code>parquet</code> Default file format for Iceberg tables: <code>parquet</code>, <code>avro</code>, or <code>orc</code> <code>debezium.sink.iceberg.allow-field-addition</code> <code>true</code> Allow field addition to target tables. Enables automatic schema evolution, expansion. <code>debezium.sink.iceberg.upsert</code> <code>false</code> Upsert mode overwrites updated rows. Any existing rows that are updated will be overwritten with the new values. Explained further below. <code>debezium.sink.iceberg.upsert-keep-deletes</code> <code>true</code> When running in upsert mode, deleted rows are marked as deleted but retained in the target table (soft delete) <code>debezium.sink.iceberg.upsert-dedup-column</code> <code>__source_ts_ns</code> With upsert mode this field is used to deduplicate data. The row with the highest <code>__source_ts_ns</code> timestamp (the latest change event) is retained. dont change! <code>debezium.sink.iceberg.upsert-op-field</code> <code>__op</code> Field name for operation type in upsert mode. dont change! <code>debezium.sink.iceberg.nested-as-variant</code> <code>false</code> When true, all nested data is stored in Iceberg variant fields, allowing schema changes to be absorbed directly within these fields. <code>debezium.sink.iceberg.create-identifier-fields</code> <code>true</code> When set to <code>false</code>, the consumer will create tables without identifier fields. This is useful for scenarios where users want to consume nested events in append-only mode. <code>debezium.sink.iceberg.table-mapper</code> <code>default-mapper</code> Mapping strategy between source table names and their corresponding Iceberg table names. Currently, only <code>default-mapper</code> implementation is provided. <code>debezium.sink.iceberg.destination-regexp</code> `` A regular expression used to modify the destination Iceberg table name. This setting allows for combining multiple tables, such as <code>table_ptt1</code> and <code>table_ptt2</code>, into a single table, <code>table_combined</code>. <code>debezium.sink.iceberg.destination-regexp-replace</code> `` Replace part of the regexp for modifying destination Iceberg table names <code>debezium.sink.iceberg.destination-uppercase-table-names</code> <code>false</code> Creates uppercase Iceberg table names <code>debezium.sink.iceberg.destination-lowercase-table-names</code> <code>false</code> Creates lowercase Iceberg table names <code>debezium.sink.iceberg.preserve-required-property</code> <code>false</code> When translating schema from source to Iceberg, by default, only primary key columns are defined as required; the rest of the columns become optional. When this is set to <code>true</code>, the columns preserve their original required/optional property. <code>debezium.sink.batch.batch-size-wait</code> <code>NoBatchSizeWait</code> Batch size wait strategy: Controls the size of data files and the frequency of uploads. Explained below. <code>debezium.sink.batch.concurrent-uploads</code> <code>1</code> The number of parallel threads to use for uploading data to Iceberg tables. When set to a value greater than 1, the consumer will write to multiple tables concurrently. <code>debezium.sink.batch.concurrent-uploads.timeout-minutes</code> <code>60</code> The maximum time in minutes to wait for all parallel uploads to complete before timing out. This is only effective when <code>debezium.sink.batch.concurrent-uploads</code> is greater than 1. <code>debezium.sink.iceberg.{iceberg.prop.name}</code> Iceberg config These Iceberg settings are passed to Iceberg without the <code>debezium.sink.iceberg.</code> prefix. <code>debezium.source.offset.storage</code> <code>io.debezium.server.iceberg.offset.IcebergOffsetBackingStore</code> The name of the Java class that is responsible for persistence of connector offsets. see debezium doc. Set to <code>io.debezium.server.iceberg.offset.IcebergOffsetBackingStore</code> to use Iceberg table <code>debezium.source.offset.storage.iceberg.table-name</code> <code>_debezium_offset_storage</code> Destination Iceberg table name to store connector offset information. <code>debezium.source.schema.history.internal</code> <code>io.debezium.server.iceberg.history.IcebergSchemaHistory</code> The name of the Java class that is responsible for persistence of the database schema history. see debezium doc. Set to <code>io.debezium.server.iceberg.history.IcebergSchemaHistory</code> to use Iceberg table. <code>debezium.source.schema.history.internal.iceberg.table-name</code> <code>_debezium_database_history_storage</code> Destination Iceberg table name to store database schema history information. <code>debezium.source.time.precision.mode</code> <code>isostring</code> Mode for temporal precision handling in Debezium source. <code>debezium.source.decimal.handling.mode</code> <code>double</code> Mode for handling decimal values in the relational database connector. <code>debezium.format.value</code> <code>connect</code> Format of the event value. one of {<code>json</code>, <code>connect</code>}. <code>debezium.format.key</code> <code>connect</code> Format of the event key. one of {<code>json</code>, <code>connect</code>}. <code>debezium.format.value.schemas.enable</code> <code>true</code> Enable schema inclusion in the event value format. <code>debezium.format.key.schemas.enable</code> <code>true</code> Enable schema inclusion in the event key format. <code>debezium.transforms</code> <code>unwrap</code> Enables Debezium transforms for event processing. <code>debezium.transforms.unwrap.type</code> <code>io.debezium.transforms.ExtractNewRecordState</code> Type of the Debezium unwrap transform. <code>debezium.transforms.unwrap.add.fields</code> <code>op,table,source.ts_ns,db,ts_ms</code> List of fields to add in the Debezium unwrap transform. <code>debezium.transforms.unwrap.delete.tombstone.handling.mode</code> <code>rewrite</code> Handling mode for delete events in the Debezium unwrap transform. <code>debezium.transforms.unwrap.drop.tombstones</code> <code>true</code> Handling mode for tombstone events (delete markers) in the Debezium unwrap transform. <code>quarkus.log.level</code> <code>INFO</code> Global Quarkus logging level"},{"location":"iceberg/#upsert-mode","title":"Upsert Mode","text":"<p>When enabled, (<code>debezium.sink.iceberg.upsert=true</code>) the consumer utilizes the source table's primary key to perform upsert operations on the target Iceberg table, effectively deleting existing rows and inserting updated ones. For tables lacking a primary key, the consumer reverts to append-only mode.</p>"},{"location":"iceberg/#upsert-mode-data-deduplication","title":"Upsert Mode Data Deduplication","text":"<p>Upsert mode enables data deduplication, prioritizing records based on their <code>__source_ts_ns</code> timestamp and operation type (<code>__op</code>). The <code>debezium.sink.iceberg.upsert-dedup-column</code> property can be used to specify a different column for deduplication (currently limited to Long type).</p> <p>Operation type priorities are as follows: <code>c</code> (create) &gt; <code>r</code> (read) &gt; <code>u</code> (update) &gt; <code>d</code> (delete). When two records with the same key and timestamp are received, the record with the higher priority operation type is retained and added to the destination table, while the duplicate record is discarded.</p>"},{"location":"iceberg/#upsert-mode-keeping-deleted-records","title":"Upsert Mode, Keeping Deleted Records","text":"<p>By default, <code>debezium.sink.iceberg.upsert-keep-deletes=true</code> retains deleted records in the Iceberg table. Setting this property to false will remove deleted records from the destination table.</p> <p>By keeping deleted records, the consumer can effectively implement soft deletes, marking records as deleted (<code>__deleted</code> set to <code>true</code>) while preserving their last known state in the Iceberg table.</p>"},{"location":"iceberg/#append-mode","title":"Append Mode","text":"<p>Default mode, when <code>debezium.sink.iceberg.upsert=false</code> switches the operation mode to append-only. In append-only mode, data deduplication is not performed, and all incoming records are appended to the destination table.</p> <p>It's important to note that even when upsert mode is enabled, tables without a primary key will switch to append-only behavior.</p>"},{"location":"iceberg/#optimizing-batch-size-or-commit-frequency","title":"Optimizing batch size (or commit frequency)","text":"<p>Debezium's real-time extraction of database events can lead to frequent commits and the creation of numerous small files, which can negatively impact performance, especially when near-real-time data feeds are sufficient.</p> <p>To address this, the connector offers various batch size wait strategies to optimize batch size and interval. These strategies introduce a delay between consumer calls, allowing for the accumulation of more events per call. This approach can significantly improve performance by reducing the number of commits and file operations.</p> <p>To fine-tune this behavior, the <code>debezium.source.max.queue.size</code> and <code>debezium.source.max.batch.size</code> properties should be configured in conjunction with the batch size wait strategy`. See the examples below.</p>"},{"location":"iceberg/#nobatchsizewait","title":"NoBatchSizeWait","text":"<p>This is default configuration by default consumer will not use any wait. All the events are consumed immediately. <code>debezium.sink.batch.batch-size-wait=NoBatchSizeWait</code></p>"},{"location":"iceberg/#maxbatchsizewait","title":"MaxBatchSizeWait","text":"<p>The MaxBatchSizeWait strategy leverages Debezium metrics to optimize batch size. It periodically checks the streaming queue size and waits until it reaches the specified <code>debezium.source.max.batch.size</code> or <code>debezium.sink.batch.batch-size-wait.max-wait-ms</code>. The maximum wait time and check interval are controlled by the <code>debezium.sink.batch.batch-size-wait.max-wait-ms</code> and <code>debezium.sink.batch.batch-size-wait.wait-interval-ms</code> properties, respectively.</p> <p>For instance, to process 2048 events per commit with a maximum wait time of 30 seconds and a check interval of 5 seconds, you would configure the settings as follows:</p> <pre><code>debezium.sink.batch.batch-size-wait=MaxBatchSizeWait\ndebezium.source.connector.class=io.debezium.connector.postgresql.PostgresConnector\ndebezium.source.max.batch.size=2048\ndebezium.source.max.queue.size=16000\ndebezium.sink.batch.batch-size-wait.max-wait-ms=30000\ndebezium.sink.batch.batch-size-wait.wait-interval-ms=5000\n</code></pre>"},{"location":"iceberg/#iceberg-table-naming-rule","title":"Iceberg Table Naming Rule","text":"<p>Iceberg tables follow a specific naming format: <code>table-namespace</code>.<code>table-prefix``database.server.name</code> <code>source_database_name</code><code>source_table_name</code></p> <p>For example:</p> <pre><code>debezium.sink.iceberg.table-namespace=default\ndatabase.server.name=testc\ndebezium.sink.iceberg.table-prefix=cdc_\n</code></pre> <p>With the configuration described above, the database table  <code>inventory.customers</code> will be replicated to the Iceberg table <code>default.testc_cdc_inventory_customers</code>.</p>"},{"location":"iceberg/#debezium-offset-storage","title":"Debezium Offset Storage","text":"<p>This implementation persists CDC offset to an Iceberg table. Debezium concurrently tracks source offset to monitor binlog position.</p> <pre><code>debezium.source.offset.storage=io.debezium.server.iceberg.offset.IcebergOffsetBackingStore\ndebezium.source.offset.storage.iceberg.table-name=debezium_offset_storage_table\n</code></pre>"},{"location":"iceberg/#debezium-database-history-storage","title":"Debezium Database History Storage","text":"<p>Stores database change history in Iceberg table.</p> <pre><code>debezium.source.database.history=io.debezium.server.iceberg.history.IcebergSchemaHistory\ndebezium.source.database.history.iceberg.table-name=debezium_database_history_storage_table\n</code></pre>"},{"location":"iceberg/#debezium-event-flattening","title":"Debezium Event Flattening","text":"<p>For an optimal experience, it is recommended to enable event flattening in the consumer configuration. For detailed information on message transformations, please refer to the Debezium documentation.</p> <p>Example Event Flattening Setting: <pre><code>debezium.transforms=unwrap\ndebezium.transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\ndebezium.transforms.unwrap.add.fields=op,table,source.ts_ns,db\ndebezium.transforms.unwrap.add.headers=db\ndebezium.transforms.unwrap.delete.tombstone.handling.mode=rewrite\n</code></pre></p> <p>When event flattening is disabled, the Iceberg consumer can only operate in append-only mode. Upsert mode and the creation of identifier fields are not supported in this configuration.</p> <p>Settings for Running the Consumer Without Event Flattening: <pre><code>debezium.sink.iceberg.upsert=false\ndebezium.sink.iceberg.create-identifier-fields=false\n</code></pre></p>"},{"location":"iceberg/#configuring-iceberg","title":"Configuring iceberg","text":"<p>All properties prefixed with <code>debezium.sink.iceberg.__ICEBERG_CONFIG__</code> are directly passed to Iceberg and hadoopConf.</p> <pre><code>debezium.sink.iceberg.{iceberg.prop.name}=xyz-value # passed to iceberg!\n</code></pre>"},{"location":"iceberg/#example-configuration","title":"Example Configuration","text":"<p>See application.properties.example</p>"},{"location":"iceberg/#automatic-schema-change-handling","title":"Automatic Schema Change Handling","text":"<p>Source systems often undergo schema changes, which can involve adding new fields, removing existing ones, or modifying the structure of existing fields. This document outlines the anticipated schema changes and how the system currently handles them.</p> <p>Important Note: While full schema evaluation is not currently supported, the system can accommodate schema expansions, such as adding new fields or expanding existing field data type. To enable this behavior, set the <code>debezium.sink.iceberg.allow-field-addition</code> configuration property to <code>true</code>.</p>"},{"location":"iceberg/#adding-new-column-to-source-table-a-column-missing-in-destination-iceberg-table","title":"Adding new column to source table (A column missing in destination iceberg table)","text":""},{"location":"iceberg/#when-debeziumsinkicebergallow-field-addition-is-false","title":"When <code>debezium.sink.iceberg.allow-field-addition</code> is <code>false</code>","text":"<p>New columns added to the source table are not automatically reflected in the destination Iceberg table. Data associated with these new columns will be ignored until the corresponding columns are manually added to the destination table schema.</p>"},{"location":"iceberg/#when-debeziumsinkicebergallow-field-addition-is-true","title":"When <code>debezium.sink.iceberg.allow-field-addition</code> is <code>true</code>","text":"<p>New columns are automatically added to the destination Iceberg table and populated with corresponding data. This behavior is handled automatically by the consumer.</p>"},{"location":"iceberg/#removing-a-column-from-the-source-table-an-extra-column-in-iceberg-table","title":"Removing a column from the source table (An extra column in iceberg table)","text":"<p>After a column is removed from the source table, the corresponding column in the destination Iceberg table will continue to exist. However, new records will have null values for that column.</p>"},{"location":"iceberg/#renaming-a-column-in-the-source-table","title":"Renaming a column in the source table","text":"<p>Renaming a column in the source table is a combination of the two scenarios described above:</p> <ol> <li>Old Column: The old column will continue to exist in the destination table, but new records will have null values    for that column.</li> <li>New Column: The new column will need to be added to the destination table schema, either automatically by the    consumer or manually. Once added, it will be populated with the appropriate data from the source.</li> </ol>"},{"location":"iceberg/#handling-mismatching-data-types","title":"Handling Mismatching Data Types","text":"<p>The system has limited support for schema changes that involve mismatching data types. While safe data type expansions, such as converting int to long, are supported, conversions that could lead to data loss, such as converting decimal to int, are not.</p>"},{"location":"iceberg/#when-debeziumsinkicebergallow-field-addition-is-true_1","title":"When <code>debezium.sink.iceberg.allow-field-addition</code> is <code>true</code>:","text":"<p>In this scenario, consumer attempts to automatically adjust the data types of destination fields to accommodate changes in the source schema. However, incompatible conversions, such as converting a float to an integer, are not supported, Consumer will throw exception. Safe conversions, such as converting an int to a double, are allowed.</p>"},{"location":"iceberg/#when-debeziumsinkicebergallow-field-addition-is-false_1","title":"When <code>debezium.sink.iceberg.allow-field-addition</code> is <code>false</code>:","text":"<p>In this scenario, the consumer attempts to convert source field values to the target data type using Jackson. If Jackson cannot successfully perform the conversion, a default value is returned by jackson.</p> <p>Jackson conversion rule for Long type: <pre><code>Method that will try to convert value of this node to a Java long. Numbers are coerced using default Java rules; booleans convert to 0 (false) and 1 (true), and Strings are parsed using default Java language integer parsing rules.\nIf representation cannot be converted to a long (including structured types like Objects and Arrays), default value of 0 will be returned; no exceptions are thrown.\n</code></pre></p> <p>Jackson conversion rule for boolean type:</p> <pre><code>Method that will try to convert value of this node to a Java boolean. JSON booleans map naturally; integer numbers other than 0 map to true, and 0 maps to false and Strings 'true' and 'false' map to corresponding values.\nIf representation can not be converted to a boolean value (including structured types like Objects and Arrays), specified defaultValue will be returned; no exceptions are thrown.\n</code></pre>"},{"location":"icebergevents/","title":"DEPRECATED","text":"<p>Using the <code>iceberg</code> consumer with the following settings is recommended to achieve the same results:</p> <pre><code># Store nested data in variant fields\ndebezium.sink.iceberg.nested-as-variant=true\n# Ensure event flattening is disabled (flattening is the default behavior)\ndebezium.transforms=,\n</code></pre>"},{"location":"icebergevents/#icebergevents-consumer","title":"<code>icebergevents</code> Consumer","text":"<p>This consumer appends all Change Data Capture (CDC) events as JSON strings to a single Iceberg table. The table is partitioned by <code>event_destination</code> and <code>event_sink_timestamptz</code> for efficient data organization and query performance.</p> <pre><code>debezium.sink.type=icebergevents\ndebezium.sink.iceberg.catalog-name=default\n</code></pre> <p>Iceberg table definition:</p> <pre><code>static final String TABLE_NAME = \"debezium_events\";\nstatic final Schema TABLE_SCHEMA = new Schema(\n        required(1, \"event_destination\", Types.StringType.get()),\n        optional(2, \"event_key\", Types.StringType.get()),\n        optional(3, \"event_value\", Types.StringType.get()),\n        optional(4, \"event_sink_epoch_ms\", Types.LongType.get()),\n        optional(5, \"event_sink_timestamptz\", Types.TimestampType.withZone())\n);\nstatic final PartitionSpec TABLE_PARTITION = PartitionSpec.builderFor(TABLE_SCHEMA)\n        .identity(\"event_destination\")\n        .hour(\"event_sink_timestamptz\")\n        .build();\nstatic final SortOrder TABLE_SORT_ORDER = SortOrder.builderFor(TABLE_SCHEMA)\n        .asc(\"event_sink_epoch_ms\", NullOrder.NULLS_LAST)\n        .build();\n</code></pre>"},{"location":"migration/","title":"Migration Guide","text":"<p>This document provides guidance on upgrading the Debezium Iceberg consumer and handling potential migration challenges.</p>"},{"location":"migration/#general-upgrade-process","title":"General Upgrade Process","text":"<p>Please be aware that each release may include backward-incompatible changes. Thorough testing in a staging environment is strongly recommended before deploying any new version to production.</p> <p>If you encounter any issues not covered here, please feel free to report them as GitHub Issues.</p>"},{"location":"migration/#handling-incompatible-data-type-changes","title":"Handling Incompatible Data Type Changes","text":"<p>An incompatible data type change can occur in two main scenarios: 1.  Upgrading the Connector: A newer version of Debezium might improve how it handles certain data types. For example, it might change its representation of timestamps from a <code>long</code> (epoch milliseconds) to a logical <code>timestamp</code> type. 2.  Source Database Schema Change: The schema in your source database might change in a way that results in an incompatible type change in the Debezium event.</p> <p>In either case, the Debezium Iceberg consumer will fail to write the new data and log an error similar to this:</p> <pre><code>java.lang.IllegalArgumentException: Cannot change column type: order_created_ts_ms: long -&gt; timestamp\n</code></pre> <p>To handle such a change, you need to perform a manual migration step on your Iceberg table. The strategy is to rename the old column, allowing the consumer to create a new column with the correct type for incoming data.</p>"},{"location":"migration/#migration-steps","title":"Migration Steps","text":"<p>Let's use the example of a column <code>order_created_ts_ms</code> changing from <code>long</code> to <code>timestamp</code>. Migrating consumer from 0.8.x to 0.9.x.</p> <ol> <li> <p>Stop the Debezium Server to prevent further write attempts.</p> </li> <li> <p>Adjust the Table Schema</p> <p>You have two primary options to resolve the schema mismatch. Choose the one that best fits your table size and operational requirements.</p> <p>Option 1: Rewrite the Table (for small tables)</p> <p>If your table is small, you can rewrite its entire contents while converting the problematic column to the new data type. This approach avoids having separate columns for old and new data but can be very expensive for large tables.</p> <p>\u26a0\ufe0f Warning: This operation rewrites the entire table and can be very slow and costly. It is generally not recommended for large production tables.</p> <p>Using Spark SQL, you can replace the table with the result of a query. The new table schema will be inferred from the <code>SELECT</code> statement.</p> <pre><code>-- Make sure to include ALL columns from the original table to avoid data loss.\nINSERT OVERWRITE my_catalog.my_db.my_table\nSELECT\n  id,\n  -- other_column_1,\n  -- other_column_2,\n  timestamp_millis(order_created_ts_ms) AS order_created_ts_ms\nFROM my_catalog.my_db.my_table;\n</code></pre> <p>Option 2: Rename the Column (Recommended for large tables)</p> <p>This is the recommended approach for most production scenarios. Renaming a column is a fast, metadata-only operation that does not require rewriting any data files. It is nearly instantaneous, making it ideal for large tables.</p> <p>You can use any tool that supports Iceberg table management, such as Spark, Flink, or the Iceberg REST catalog API.</p> <p>Using Spark SQL: <pre><code>ALTER TABLE my_catalog.my_db.my_table RENAME COLUMN order_created_ts_ms TO order_created_ts_ms_legacy;\n</code></pre></p> </li> <li> <p>Upgrade and Restart the Debezium Server.</p> </li> </ol>"},{"location":"migration/#what-happens-next","title":"What Happens Next?","text":"<p>When the consumer processes the new events, it will find that the <code>order_created_ts_ms</code> column no longer exists. It will then add it to the table schema as a new column with the correct <code>timestamp</code> type.</p> <p>After this process, your table will have both columns: - <code>order_created_ts_ms_legacy</code> (<code>long</code>): Contains the old data. New rows will have <code>null</code> in this column. - <code>order_created_ts_ms</code> (<code>timestamp</code>): Contains the new data. Old rows will have <code>null</code> in this column.</p> <p>This approach preserves all your data while allowing the schema to evolve to accommodate the new data type. You can later decide to backfill the data and consolidate it into a single column if needed.</p> <p>or you can simply could use COALESCE and read consolidated data <pre><code>SELECT COALESCE(timestamp_millis(order_created_ts_ms_legacy), order_created_ts_ms) AS order_created_ts_ms FROM my_catalog.my_db.my_table\n</code></pre></p>"},{"location":"python-runner/","title":"Python Runner for Debezium Server","text":"<p>It's possible to use python to run,operate debezium server</p> <p>For convenience this project additionally provides Python scripts to automate the startup, shutdown, and configuration of Debezium Server. Using Python, you can do various Debezium Server operation and take programmatic, dynamic, debezium configuration. example:</p> <pre><code>pip install git+https://github.com/memiiso/debezium-server-iceberg.git@master#subdirectory=python\ndebezium\n# running with custom arguments\ndebezium --debezium_dir=/my/debezium_server/dir/ --java_home=/my/java/homedir/\n</code></pre> <pre><code>from debezium import Debezium\n\nd = Debezium(debezium_dir=\"/dbz/server/dir\", java_home='/java/home/dir')\njava_args = []\njava_args.append(\"-Dquarkus.log.file.enable=true\")\njava_args.append(\"-Dquarkus.log.file.path=/logs/dbz_logfile.log\")\nd.run(*java_args)\n</code></pre> <pre><code>import os\nfrom debezium import DebeziumRunAsyn\n\njava_args = []\n# using python we can dynamically influence debezium \n# by chaning its config within python\nif my_custom_condition_check is True:\n    # Option 1: set config using java arg\n    java_args.append(\"-Dsnapshot.mode=always\")\n    # Option 2: set config using ENV variable\n    os.environ[\"SNAPSHOT_MODE\"] = \"always\"\n\njava_args.append(\"-Dquarkus.log.file.enable=true\")\njava_args.append(\"-Dquarkus.log.file.path=/logs/dbz_logfile.log\")\nd = DebeziumRunAsyn(debezium_dir=\"/dbz/server/dir\", java_home='/java/home/dir', java_args=java_args)\nd.run()\nd.join()\n</code></pre>"}]}