{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'lakekeeper' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"iceberg_warehouse\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.6.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_config = SparkConf().setMaster('local').setAppName(\"Iceberg-REST\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Write Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|lakekeeper   |\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "Current default catalog: lakekeeper\n",
      "+-------------------+\n",
      "|namespace          |\n",
      "+-------------------+\n",
      "|icebergdata        |\n",
      "|pyiceberg_namespace|\n",
      "|my_namespace       |\n",
      "+-------------------+\n",
      "\n",
      "+-----------+-------------------------------------------+-----------+\n",
      "|namespace  |tableName                                  |isTemporary|\n",
      "+-----------+-------------------------------------------+-----------+\n",
      "|icebergdata|debezium_offset_storage_table              |false      |\n",
      "|icebergdata|debeziumcdc_dbz__inventory_customers       |false      |\n",
      "|icebergdata|debeziumcdc_dbz__inventory_orders          |false      |\n",
      "|icebergdata|debeziumcdc_dbz__inventory_products        |false      |\n",
      "|icebergdata|debeziumcdc_dbz__inventory_products_on_hand|false      |\n",
      "|icebergdata|debeziumcdc_dbz__inventory_geom            |false      |\n",
      "+-----------+-------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)\n",
    "print(f\"Current default catalog: {spark.catalog.currentCatalog()}\")\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "spark.sql(\"SHOW TABLES in icebergdata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+--------+----------+---------+----+-------+-------------------+--------+\n",
      "|id   |order_date|purchaser|quantity|product_id|__deleted|__op|__table|__source_ts_ns     |__db    |\n",
      "+-----+----------+---------+--------+----------+---------+----+-------+-------------------+--------+\n",
      "|10333|2025-06-17|1231     |8       |105       |false    |c   |orders |1762545825949497000|postgres|\n",
      "|10332|2025-04-18|1230     |3       |109       |false    |c   |orders |1762545815928545000|postgres|\n",
      "|10331|2025-07-18|1229     |3       |104       |false    |c   |orders |1762545805910891000|postgres|\n",
      "|10330|2025-01-07|1229     |6       |102       |false    |c   |orders |1762545805909349000|postgres|\n",
      "|10329|2025-08-01|1229     |9       |109       |false    |c   |orders |1762545805907815000|postgres|\n",
      "|10328|2025-03-15|1228     |8       |101       |false    |c   |orders |1762545795885726000|postgres|\n",
      "|10327|2025-08-16|1228     |9       |104       |false    |c   |orders |1762545795884002000|postgres|\n",
      "|10326|2025-01-31|1227     |4       |101       |false    |c   |orders |1762545785870437000|postgres|\n",
      "|10325|2025-08-28|1225     |8       |102       |false    |c   |orders |1762545765814665000|postgres|\n",
      "|10324|2025-10-28|1225     |10      |108       |false    |c   |orders |1762545765813196000|postgres|\n",
      "|10323|2025-05-04|1225     |2       |107       |false    |c   |orders |1762545765811746000|postgres|\n",
      "|10322|2025-09-09|1222     |8       |107       |false    |c   |orders |1762545735728532000|postgres|\n",
      "|10321|2025-04-25|1222     |6       |103       |false    |c   |orders |1762545735725738000|postgres|\n",
      "|10320|2025-06-18|1222     |3       |103       |false    |c   |orders |1762545735723009000|postgres|\n",
      "|10319|2025-08-28|1219     |10      |103       |false    |c   |orders |1762545705668550000|postgres|\n",
      "|10318|2025-03-28|1219     |6       |103       |false    |c   |orders |1762545705667263000|postgres|\n",
      "|10317|2025-07-27|1219     |2       |108       |false    |c   |orders |1762545705665261000|postgres|\n",
      "|10316|2025-04-17|1218     |9       |104       |false    |c   |orders |1762545695651266000|postgres|\n",
      "|10315|2025-09-30|1218     |8       |105       |false    |c   |orders |1762545695649796000|postgres|\n",
      "|10314|2025-08-09|1217     |5       |108       |false    |c   |orders |1762545685635431000|postgres|\n",
      "+-----+----------+---------+--------+----------+---------+----+-------+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve and print orders data\n",
    "orders = spark.sql(\"select * from icebergdata.debeziumcdc_dbz__inventory_orders order by id desc\")\n",
    "orders.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and print customers data\n",
    "# customers = spark.sql(\"select id,input_file_name() as input_file  from debeziumevents.debeziumcdc_testc_inventory_customers\")\n",
    "customers = spark.sql(\"select *  from icebergdata.debeziumcdc_testc_inventory_customers order by 1 asc\")\n",
    "customers.limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and print offset\n",
    "data = spark.sql(\"select * from icebergdata.debezium_offset_storage_table\")\n",
    "data.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and print offset\n",
    "data = spark.sql(\n",
    "    \"select * from icebergdata.debeziumcdc_testc_inventory_customers.history ORDER BY made_current_at DESC\")\n",
    "data.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and print offset\n",
    "data = spark.sql(\"select * from icebergdata.debeziumcdc_testc_inventory_customers.snapshots\")\n",
    "data.limit(4).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
